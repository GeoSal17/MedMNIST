{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0463a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras_tuner as kt\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "import medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87823022",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGAMOS EL DATASET BREASTMNIST\n",
    "\n",
    "#data_flag = 'pathmnist'\n",
    "data_flag = 'breastmnist'\n",
    "download = True\n",
    "\n",
    "class TrainingConfig:\n",
    "    EPOCHS:        int = 50\n",
    "    BATCH_SIZE:    int = 256\n",
    "    LEARNING_RATE: float = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96758bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SACAMOS LA INFO, COLUMNAS Y CLASES Y ASÍ\n",
    "\n",
    "info = medmnist.INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59048ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info)\n",
    "\n",
    "print(\"\\n'task': 'binary-class', 'label': {'0': 'malignant', '1': 'normal, benign'}, 'n_channels': 1, 'n_samples': {'train': 546, 'val': 78, 'test': 156}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fae3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "val_dataset = DataClass(split='val', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=TrainingConfig.BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*TrainingConfig.BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*TrainingConfig.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(\"===================\")\n",
    "print(test_dataset)\n",
    "print(\"===================\")\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.montage(length=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential(\n",
    "  [\n",
    "    Input(shape=(28, 28, 1)),\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(.1),\n",
    "    layers.RandomZoom(.2)\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58891bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential(name=\"modelo_cnn\")\n",
    "\n",
    "    #CANTIDAD DE FILTROS\n",
    "    hp_filters_1 = hp.Choice('filters_block1', values=[16, 32, 64])\n",
    "    hp_filters_2 = hp.Choice('filters_block2', values=[16, 32, 64, 128])\n",
    "    \n",
    "    #NÚMERO DE NEURONAS EN CAPA DENSA\n",
    "    hp_units = hp.Int('dense_units', min_value=256, max_value=512, step=128)\n",
    "    \n",
    "    #TAZA DE APRENDIZAJE\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    #TAZA DE DROPOUT\n",
    "    hp_dropout_dense = hp.Float('dropout_dense', min_value=0.2, max_value=0.5, step=0.1)\n",
    "\n",
    "    #OPTIMIZADORES\n",
    "    hp_optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd', 'adagrad'])\n",
    "\n",
    "    #LLAMAR AL OPTIMIZADOR Y DARLE EL LR\n",
    "    if hp_optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=hp_learning_rate)\n",
    "    elif hp_optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=hp_learning_rate)\n",
    "    elif hp_optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=hp_learning_rate)\n",
    "    else:\n",
    "        optimizer = Adagrad(learning_rate=hp_learning_rate)\n",
    "\n",
    "    #CONSTRUYENDO MODELO\n",
    "    model.add(data_augmentation)\n",
    "    model.add(Conv2D(filters=hp_filters_1, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=hp_filters_1, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25)) \n",
    "    \n",
    "    model.add(Conv2D(filters=hp_filters_2, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=hp_filters_2, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp_units, activation='relu'))\n",
    "    model.add(Dropout(rate=hp_dropout_dense))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "   \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c9dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SINTONIZADOR\n",
    "tuner = kt.GridSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=100,  #NUM DE CONFIGURACIONES DIFERENTES\n",
    "    executions_per_trial=1, #CANT DE VECES QUE SE ENTRENA CADA CONFIG\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='breast_cancer_optimization',\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ce423",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=10, verbose=2)\n",
    "reduce_lr = ReduceLROnPlateau( monitor = 'val_loss', factor = 0.1, patience = 8, min_lr = 0.00001, verbose = 2, mode='min' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b34bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARAMOS LABELS DE LAS IMAGENES\n",
    "\n",
    "X_train = train_dataset.imgs\n",
    "y_train = train_dataset.labels\n",
    "\n",
    "X_val = val_dataset.imgs\n",
    "y_val = val_dataset.labels\n",
    "\n",
    "X_test = test_dataset.imgs\n",
    "y_test = test_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape, X_train.shape)\n",
    "print(\"===================\")\n",
    "print(y_val.shape, X_val.shape)\n",
    "print(\"===================\")\n",
    "print(y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d10491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AJUSTAMOS TAMAÑO PARA QUE TENGA LOS REQUISITOS QUE PIDE EL MODELO\n",
    "X_train_expanded = np.expand_dims(X_train, -1)  #(546, 28, 28, 1) \n",
    "X_val_expanded = np.expand_dims(X_val, -1) #(78, 28, 28, 1) \n",
    "X_test_expanded = np.expand_dims(X_test, -1) #(156, 28, 28, 1) \n",
    "\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "\n",
    "print(X_train_expanded.shape, y_train.shape)\n",
    "print(\"===================\")\n",
    "print(X_val_expanded.shape, y_val.shape)\n",
    "print(\"===================\")\n",
    "print(X_test_expanded.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99208988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Se inicia la búsqueda de los mejores hp\")\n",
    "hist = tuner.search(X_train_expanded,\n",
    "             y_train,\n",
    "             epochs=TrainingConfig.EPOCHS,\n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[reduce_lr, stop_early]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBTENEMOS LOS MEJORES HIPERPARAMETROS\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Los mejores hiperparámetros son:\n",
    "- Filtros Bloque 1: {best_hps.get('filters_block1')}\n",
    "- Filtros Bloque 2: {best_hps.get('filters_block2')}\n",
    "- Unidades Densas: {best_hps.get('dense_units')}\n",
    "- Tasa de Aprendizaje: {best_hps.get('learning_rate')}\n",
    "- Tasa de Dropout: {best_hps.get('dropout_dense'):.2f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARMAMOS EL MODELO CON LOS MEJORES HP\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6905617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUARDAMOS EL MODELO\n",
    "#best_model.save('modelo_cancer_mama_final.keras')\n",
    "#best_model.save('modelo_cancer_mama2.keras')\n",
    "best_model.save('modelo_data_augmentation.keras')\n",
    "\n",
    "#PARA CARGARLO\n",
    "# from tensorflow.keras.models import load_model\n",
    "# loaded_model = load_model('modelo_cancer_mama_final.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRENAMOS\n",
    "historial = best_model.fit(\n",
    "    X_train_expanded,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[stop_early, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f10004",
   "metadata": {},
   "source": [
    "## Gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdfc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter, MultipleLocator\n",
    "\n",
    "def plot_results(metrics, title=None, ylabel=None, ylim=None, metric_name=None, color=None):\n",
    "     \n",
    "    fig, ax = plt.subplots(figsize=(15, 4))\n",
    " \n",
    "    if not (isinstance(metric_name, list) or isinstance(metric_name, tuple)):\n",
    "        metrics = [metrics,]\n",
    "        metric_name = [metric_name,]\n",
    "         \n",
    "    for idx, metric in enumerate(metrics):    \n",
    "        ax.plot(metric, color=color[idx])\n",
    "     \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.xlim([0, TrainingConfig.EPOCHS-1])\n",
    "    plt.ylim(ylim)\n",
    "    # Tailor x-axis tick marks\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "    plt.grid(True)\n",
    "    plt.legend(metric_name)   \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = historial.history[\"loss\"]\n",
    "train_acc  = historial.history[\"accuracy\"]\n",
    "valid_loss = historial.history[\"val_loss\"]\n",
    "valid_acc  = historial.history[\"val_accuracy\"]\n",
    "   \n",
    "plot_results([ train_loss, valid_loss ],        \n",
    "            ylabel=\"Loss\", \n",
    "            #ylim = [0.0, 5.0],\n",
    "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
    "            color=[\"g\", \"b\"]);\n",
    " \n",
    "plot_results([ train_acc, valid_acc ], \n",
    "            ylabel=\"Accuracy\",\n",
    "            #ylim = [0.0, 1.0],\n",
    "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
    "            color=[\"g\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a153dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    #plt.ylim((0,1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(historial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_loss(hist):\n",
    "    plt.plot(hist.history[\"loss\"],'.r')\n",
    "    plt.plot(hist.history[\"val_loss\"],'*b')\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    #plt.ylim((0,1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_hist_loss(historial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae413127",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_test = best_model.predict(X_test_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, count_elements = np.unique(np.round(prediccion_test),return_counts=True)\n",
    "print(unique_elements)\n",
    "print(count_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_test=prediccion_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96368a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.zeros(X_test.shape[0])\n",
    "\n",
    "for id in range(X_test_expanded.shape[0]):\n",
    "    pred_test[id] = np.round( prediccion_test[id] )\n",
    "\n",
    "#pred_test = (prediccion_test > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bf872",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = confusion_matrix(y_test, pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = con, display_labels=['Maligno', 'No Maligno'])\n",
    "disp.plot()\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n",
    "\n",
    "#GENTE, TENEMOS CLASES DESBALANCEADAS AAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULAMOS LOS VALORES DE RENDIMIENTO EN EL ENTRENAMIENTO\n",
    "final_loss = historial.history['loss'][-1]\n",
    "final_accuracy = historial.history['accuracy'][-1]\n",
    "final_val_loss = historial.history['val_loss'][-1]\n",
    "final_val_accuracy = historial.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Resultado Final del Entrenamiento:\")\n",
    "print(f\"  - Pérdida de Entrenamiento (Loss): {final_loss:.4f}\")\n",
    "print(f\"  - Precisión de Entrenamiento (Accuracy): {final_accuracy:.4f}\")\n",
    "print(f\"  - Pérdida de Validación (Validation Loss): {final_val_loss:.4f}\")\n",
    "print(f\"  - Precisión de Validación (Validation Accuracy): {final_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052aecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUAMOS EL RENDIMIENTO DEL MODELO CON TEST\n",
    "print(\"Evaluando el modelo final con Test\")\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nResultados Finales en el Test Set:\")\n",
    "print(f\"  - Pérdida (Loss): {test_loss:.4f}\")\n",
    "print(f\"  - Precisión (Accuracy): {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc7770",
   "metadata": {},
   "source": [
    "### Resultado de la versión 1\n",
    "Resultado Final del Entrenamiento:\n",
    "  - Pérdida de Entrenamiento (Loss): 0.1904\n",
    "  - Precisión de Entrenamiento (Accuracy): 0.9346\n",
    "  - Pérdida de Validación (Validation Loss): 0.4545\n",
    "  - Precisión de Validación (Validation Accuracy): 0.7866"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431c213",
   "metadata": {},
   "source": [
    "### Resultado de la versión 2 \n",
    "Se añadieron las capas de output para arreglar el sobreajuste.\n",
    "\n",
    "Resultado Final del Entrenamiento:\n",
    "  - Pérdida de Entrenamiento (Loss): 0.5360\n",
    "  - Precisión de Entrenamiento (Accuracy): 0.7408\n",
    "  - Pérdida de Validación (Validation Loss): 0.5447\n",
    "  - Precisión de Validación (Validation Accuracy): 0.7439"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f609c",
   "metadata": {},
   "source": [
    "### Resultado de la versión 3\n",
    "\n",
    "Integré el Keras Tuner para buscar los mejores hiperparámetros, agregué más evaluaciones y la matriz de confusión. También agregué la evaluación real con el dataset de test. En resumen: muchos cambios :)\n",
    "\n",
    "Resultado Final del Entrenamiento:\n",
    "  - Pérdida de Entrenamiento (Loss): 0.0337\n",
    "  - Precisión de Entrenamiento (Accuracy): 0.9835 *(hay un poco de sobreajuste)*\n",
    "  - Pérdida de Validación (Validation Loss): 0.7607 \n",
    "  - Precisión de Validación (Validation Accuracy): 0.9103\n",
    "\n",
    "Resultados Finales en el Test Set:\n",
    "  - Pérdida (Loss): 1.1189\n",
    "  - Precisión (Accuracy): 0.8590"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab678f57",
   "metadata": {},
   "source": [
    "#### Resultado de la version 3.5\n",
    "\n",
    "Se integraron los cambios recomendados por Edelmira, cambios en el learning rate, se implementó otro callback, se modificaron las capas y las neuronas. \n",
    "\n",
    "Resultado Final del Entrenamiento:\n",
    "  - Pérdida de Entrenamiento (Loss): 0.0442\n",
    "  - Precisión de Entrenamiento (Accuracy): 0.9872\n",
    "  - Pérdida de Validación (Validation Loss): 0.3741\n",
    "  - Precisión de Validación (Validation Accuracy): 0.9359\n",
    "\n",
    "Resultados Finales en el Test Set:\n",
    "  - Pérdida (Loss): 0.7562\n",
    "  - Precisión (Accuracy): 0.8974"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20556dc7",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d3244",
   "metadata": {},
   "source": [
    "# ESTO AÚN NO ESTÁ LISTO!!! ↓↓↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self, in_channels, num_classes):\n",
    "#         super(Net, self).__init__()\n",
    "\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU())\n",
    "\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 16, kernel_size=3),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 64, kernel_size=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU())\n",
    "        \n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, kernel_size=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU())\n",
    "\n",
    "#         self.layer5 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(64 * 4 * 4, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, num_classes))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = self.layer5(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c69921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Net(in_channels=n_channels, num_classes=n_classes)\n",
    "\n",
    "# if task == \"multi-label, binary-class\":\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "# else:\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7dc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(TrainingConfig.NUM_EPOCHS):\n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "    \n",
    "#     model.train()\n",
    "#     for inputs, targets in tqdm(train_loader):\n",
    "#         # forward + backward + optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         if task == 'multi-label, binary-class':\n",
    "#             targets = targets.to(torch.float32)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#         else:\n",
    "#             targets = targets.squeeze().long()\n",
    "#             loss = criterion(outputs, targets)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from medmnist import Evaluator\n",
    "\n",
    "# def test(split):\n",
    "#     model.eval()\n",
    "#     y_true = torch.tensor([])\n",
    "#     y_score = torch.tensor([])\n",
    "    \n",
    "#     data_loader = train_loader_at_eval if split == 'train' else test_loader\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in data_loader:\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             if task == 'multi-label, binary-class':\n",
    "#                 targets = targets.to(torch.float32)\n",
    "#                 outputs = outputs.softmax(dim=-1)\n",
    "#             else:\n",
    "#                 targets = targets.squeeze().long()\n",
    "#                 outputs = outputs.softmax(dim=-1)\n",
    "#                 targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "#             y_true = torch.cat((y_true, targets), 0)\n",
    "#             y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "#         y_true = y_true.numpy()\n",
    "#         y_score = y_score.detach().numpy()\n",
    "        \n",
    "#         evaluator = Evaluator(data_flag, split)\n",
    "#         metrics = evaluator.evaluate(y_score)\n",
    "    \n",
    "#         print('%s  auc: %.3f  acc:%.3f' % (split, *metrics)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('==> Evaluating ...')\n",
    "# test('train')\n",
    "# test('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259739d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "#     model.add( Input(  shape=(X_train_robustscaler.shape[1],)) )\n",
    "\n",
    "#     for i in range(hp.Int(\"num_layers\", 1, 2)):  # Tune the number of layers.\n",
    "#         model.add(\n",
    "#             Dense(\n",
    "#                 units      = hp.Int   (f'units_{i}', min_value=1, max_value=25, step=2 ), # <--- [1..15]\n",
    "#                 activation = hp.Choice(f\"activation_{i}\", [\"relu\", \"selu\",\"leaky_relu\"]),\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     model.add( Dense( 1, activation = 'sigmoid', name = \"predictions\" ) ) # <---- capa de salida\n",
    "\n",
    "#     lr = hp.Choice( 'lr', values=[1e-2, 1e-3, 1e-4] )\n",
    "#     optimizers_dict = {\n",
    "#        \"Adam\":    keras.optimizers.Adam(learning_rate=lr),\n",
    "#         \"SGD\":     keras.optimizers.SGD(learning_rate=lr),\n",
    "#         \"Adagrad\": keras.optimizers.Adagrad(learning_rate=lr)\n",
    "#         }\n",
    "\n",
    "#     hp_optimizers = hp.Choice(\n",
    "#         'optimizer',\n",
    "#         values=[ \"SGD\", \"Adam\", \"Adagrad\"]\n",
    "#         )\n",
    "\n",
    "#     model.compile( optimizer    = optimizers_dict[hp_optimizers],\n",
    "#                     loss      = \"binary_crossentropy\",\n",
    "#                     metrics   = ['accuracy']\n",
    "#                     )\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69855f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PROTOTIPO DE MODELO HELP \n",
    "# # AL FINAL ES SIGMOIDE DUH\n",
    "\n",
    "# def cnn_model(input_shape=(28, 28, 1)):\n",
    "     \n",
    "#     model = Sequential()\n",
    "     \n",
    "#     #------------------------------------\n",
    "#     # Conv Block 1: 32 Filters, MaxPool.\n",
    "#     #------------------------------------\n",
    "#     model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=input_shape))\n",
    "#     model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25)) #AGREGUE LAYERS DE OUTPUT PARA REDUCIR EL SOBREAJUSTE\n",
    " \n",
    "#     #------------------------------------\n",
    "#     # Conv Block 2: 64 Filters, MaxPool.\n",
    "#     #------------------------------------\n",
    "#     model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "#     model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    " \n",
    "#     #------------------------------------\n",
    "#     # Conv Block 3: 64 Filters, MaxPool.\n",
    "#     #------------------------------------\n",
    "#     model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "#     model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25)) \n",
    "     \n",
    "#     #------------------------------------\n",
    "#     # Flatten the convolutional features.\n",
    "#     #------------------------------------\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dropout(0.5)) #Dropout más denso para el final\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "#     return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
